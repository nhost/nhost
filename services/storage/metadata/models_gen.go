// Code generated by github.com/99designs/gqlgen, DO NOT EDIT.

package metadata

import (
	"bytes"
	"fmt"
	"io"
	"strconv"
	"time"
)

// Boolean expression to compare columns of type "Boolean". All fields are combined with logical 'AND'.
type BooleanComparisonExp struct {
	Eq     *bool  `json:"_eq,omitempty"`
	Gt     *bool  `json:"_gt,omitempty"`
	Gte    *bool  `json:"_gte,omitempty"`
	In     []bool `json:"_in,omitempty"`
	IsNull *bool  `json:"_is_null,omitempty"`
	Lt     *bool  `json:"_lt,omitempty"`
	Lte    *bool  `json:"_lte,omitempty"`
	Neq    *bool  `json:"_neq,omitempty"`
	Nin    []bool `json:"_nin,omitempty"`
}

// Boolean expression to compare columns of type "Int". All fields are combined with logical 'AND'.
type IntComparisonExp struct {
	Eq     *int64  `json:"_eq,omitempty"`
	Gt     *int64  `json:"_gt,omitempty"`
	Gte    *int64  `json:"_gte,omitempty"`
	In     []int64 `json:"_in,omitempty"`
	IsNull *bool   `json:"_is_null,omitempty"`
	Lt     *int64  `json:"_lt,omitempty"`
	Lte    *int64  `json:"_lte,omitempty"`
	Neq    *int64  `json:"_neq,omitempty"`
	Nin    []int64 `json:"_nin,omitempty"`
}

// Boolean expression to compare columns of type "String". All fields are combined with logical 'AND'.
type StringComparisonExp struct {
	Eq  *string `json:"_eq,omitempty"`
	Gt  *string `json:"_gt,omitempty"`
	Gte *string `json:"_gte,omitempty"`
	// does the column match the given case-insensitive pattern
	Ilike *string  `json:"_ilike,omitempty"`
	In    []string `json:"_in,omitempty"`
	// does the column match the given POSIX regular expression, case insensitive
	Iregex *string `json:"_iregex,omitempty"`
	IsNull *bool   `json:"_is_null,omitempty"`
	// does the column match the given pattern
	Like *string `json:"_like,omitempty"`
	Lt   *string `json:"_lt,omitempty"`
	Lte  *string `json:"_lte,omitempty"`
	Neq  *string `json:"_neq,omitempty"`
	// does the column NOT match the given case-insensitive pattern
	Nilike *string  `json:"_nilike,omitempty"`
	Nin    []string `json:"_nin,omitempty"`
	// does the column NOT match the given POSIX regular expression, case insensitive
	Niregex *string `json:"_niregex,omitempty"`
	// does the column NOT match the given pattern
	Nlike *string `json:"_nlike,omitempty"`
	// does the column NOT match the given POSIX regular expression, case sensitive
	Nregex *string `json:"_nregex,omitempty"`
	// does the column NOT match the given SQL regular expression
	Nsimilar *string `json:"_nsimilar,omitempty"`
	// does the column match the given POSIX regular expression, case sensitive
	Regex *string `json:"_regex,omitempty"`
	// does the column match the given SQL regular expression
	Similar *string `json:"_similar,omitempty"`
}

// columns and relationships of "storage.buckets"
type Buckets struct {
	CacheControl       *string   `json:"cacheControl,omitempty"`
	CreatedAt          time.Time `json:"createdAt"`
	DownloadExpiration int64     `json:"downloadExpiration"`
	// An array relationship
	Files []*Files `json:"files"`
	// An aggregate relationship
	FilesAggregate       *FilesAggregate `json:"files_aggregate"`
	ID                   string          `json:"id"`
	MaxUploadFileSize    int64           `json:"maxUploadFileSize"`
	MinUploadFileSize    int64           `json:"minUploadFileSize"`
	PresignedUrlsEnabled bool            `json:"presignedUrlsEnabled"`
	UpdatedAt            time.Time       `json:"updatedAt"`
}

// aggregated selection of "storage.buckets"
type BucketsAggregate struct {
	Aggregate *BucketsAggregateFields `json:"aggregate,omitempty"`
	Nodes     []*Buckets              `json:"nodes"`
}

// aggregate fields of "storage.buckets"
type BucketsAggregateFields struct {
	Avg        *BucketsAvgFields        `json:"avg,omitempty"`
	Count      int64                    `json:"count"`
	Max        *BucketsMaxFields        `json:"max,omitempty"`
	Min        *BucketsMinFields        `json:"min,omitempty"`
	Stddev     *BucketsStddevFields     `json:"stddev,omitempty"`
	StddevPop  *BucketsStddevPopFields  `json:"stddev_pop,omitempty"`
	StddevSamp *BucketsStddevSampFields `json:"stddev_samp,omitempty"`
	Sum        *BucketsSumFields        `json:"sum,omitempty"`
	VarPop     *BucketsVarPopFields     `json:"var_pop,omitempty"`
	VarSamp    *BucketsVarSampFields    `json:"var_samp,omitempty"`
	Variance   *BucketsVarianceFields   `json:"variance,omitempty"`
}

// aggregate avg on columns
type BucketsAvgFields struct {
	DownloadExpiration *float64 `json:"downloadExpiration,omitempty"`
	MaxUploadFileSize  *float64 `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *float64 `json:"minUploadFileSize,omitempty"`
}

// Boolean expression to filter rows from the table "storage.buckets". All fields are combined with a logical 'AND'.
type BucketsBoolExp struct {
	And                  []*BucketsBoolExp         `json:"_and,omitempty"`
	Not                  *BucketsBoolExp           `json:"_not,omitempty"`
	Or                   []*BucketsBoolExp         `json:"_or,omitempty"`
	CacheControl         *StringComparisonExp      `json:"cacheControl,omitempty"`
	CreatedAt            *TimestamptzComparisonExp `json:"createdAt,omitempty"`
	DownloadExpiration   *IntComparisonExp         `json:"downloadExpiration,omitempty"`
	Files                *FilesBoolExp             `json:"files,omitempty"`
	FilesAggregate       *FilesAggregateBoolExp    `json:"files_aggregate,omitempty"`
	ID                   *StringComparisonExp      `json:"id,omitempty"`
	MaxUploadFileSize    *IntComparisonExp         `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize    *IntComparisonExp         `json:"minUploadFileSize,omitempty"`
	PresignedUrlsEnabled *BooleanComparisonExp     `json:"presignedUrlsEnabled,omitempty"`
	UpdatedAt            *TimestamptzComparisonExp `json:"updatedAt,omitempty"`
}

// input type for incrementing numeric columns in table "storage.buckets"
type BucketsIncInput struct {
	DownloadExpiration *int64 `json:"downloadExpiration,omitempty"`
	MaxUploadFileSize  *int64 `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *int64 `json:"minUploadFileSize,omitempty"`
}

// input type for inserting data into table "storage.buckets"
type BucketsInsertInput struct {
	CacheControl         *string                 `json:"cacheControl,omitempty"`
	CreatedAt            *time.Time              `json:"createdAt,omitempty"`
	DownloadExpiration   *int64                  `json:"downloadExpiration,omitempty"`
	Files                *FilesArrRelInsertInput `json:"files,omitempty"`
	ID                   *string                 `json:"id,omitempty"`
	MaxUploadFileSize    *int64                  `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize    *int64                  `json:"minUploadFileSize,omitempty"`
	PresignedUrlsEnabled *bool                   `json:"presignedUrlsEnabled,omitempty"`
	UpdatedAt            *time.Time              `json:"updatedAt,omitempty"`
}

// aggregate max on columns
type BucketsMaxFields struct {
	CacheControl       *string    `json:"cacheControl,omitempty"`
	CreatedAt          *time.Time `json:"createdAt,omitempty"`
	DownloadExpiration *int64     `json:"downloadExpiration,omitempty"`
	ID                 *string    `json:"id,omitempty"`
	MaxUploadFileSize  *int64     `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *int64     `json:"minUploadFileSize,omitempty"`
	UpdatedAt          *time.Time `json:"updatedAt,omitempty"`
}

// aggregate min on columns
type BucketsMinFields struct {
	CacheControl       *string    `json:"cacheControl,omitempty"`
	CreatedAt          *time.Time `json:"createdAt,omitempty"`
	DownloadExpiration *int64     `json:"downloadExpiration,omitempty"`
	ID                 *string    `json:"id,omitempty"`
	MaxUploadFileSize  *int64     `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *int64     `json:"minUploadFileSize,omitempty"`
	UpdatedAt          *time.Time `json:"updatedAt,omitempty"`
}

// response of any mutation on the table "storage.buckets"
type BucketsMutationResponse struct {
	// number of rows affected by the mutation
	AffectedRows int64 `json:"affected_rows"`
	// data from the rows affected by the mutation
	Returning []*Buckets `json:"returning"`
}

// input type for inserting object relation for remote table "storage.buckets"
type BucketsObjRelInsertInput struct {
	Data *BucketsInsertInput `json:"data"`
	// upsert condition
	OnConflict *BucketsOnConflict `json:"on_conflict,omitempty"`
}

// on_conflict condition type for table "storage.buckets"
type BucketsOnConflict struct {
	Constraint    BucketsConstraint     `json:"constraint"`
	UpdateColumns []BucketsUpdateColumn `json:"update_columns"`
	Where         *BucketsBoolExp       `json:"where,omitempty"`
}

// Ordering options when selecting data from "storage.buckets".
type BucketsOrderBy struct {
	CacheControl         *OrderBy               `json:"cacheControl,omitempty"`
	CreatedAt            *OrderBy               `json:"createdAt,omitempty"`
	DownloadExpiration   *OrderBy               `json:"downloadExpiration,omitempty"`
	FilesAggregate       *FilesAggregateOrderBy `json:"files_aggregate,omitempty"`
	ID                   *OrderBy               `json:"id,omitempty"`
	MaxUploadFileSize    *OrderBy               `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize    *OrderBy               `json:"minUploadFileSize,omitempty"`
	PresignedUrlsEnabled *OrderBy               `json:"presignedUrlsEnabled,omitempty"`
	UpdatedAt            *OrderBy               `json:"updatedAt,omitempty"`
}

// primary key columns input for table: storage.buckets
type BucketsPkColumnsInput struct {
	ID string `json:"id"`
}

// input type for updating data in table "storage.buckets"
type BucketsSetInput struct {
	CacheControl         *string    `json:"cacheControl,omitempty"`
	CreatedAt            *time.Time `json:"createdAt,omitempty"`
	DownloadExpiration   *int64     `json:"downloadExpiration,omitempty"`
	ID                   *string    `json:"id,omitempty"`
	MaxUploadFileSize    *int64     `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize    *int64     `json:"minUploadFileSize,omitempty"`
	PresignedUrlsEnabled *bool      `json:"presignedUrlsEnabled,omitempty"`
	UpdatedAt            *time.Time `json:"updatedAt,omitempty"`
}

// aggregate stddev on columns
type BucketsStddevFields struct {
	DownloadExpiration *float64 `json:"downloadExpiration,omitempty"`
	MaxUploadFileSize  *float64 `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *float64 `json:"minUploadFileSize,omitempty"`
}

// aggregate stddev_pop on columns
type BucketsStddevPopFields struct {
	DownloadExpiration *float64 `json:"downloadExpiration,omitempty"`
	MaxUploadFileSize  *float64 `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *float64 `json:"minUploadFileSize,omitempty"`
}

// aggregate stddev_samp on columns
type BucketsStddevSampFields struct {
	DownloadExpiration *float64 `json:"downloadExpiration,omitempty"`
	MaxUploadFileSize  *float64 `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *float64 `json:"minUploadFileSize,omitempty"`
}

// Streaming cursor of the table "buckets"
type BucketsStreamCursorInput struct {
	// Stream column input with initial value
	InitialValue *BucketsStreamCursorValueInput `json:"initial_value"`
	// cursor ordering
	Ordering *CursorOrdering `json:"ordering,omitempty"`
}

// Initial value of the column from where the streaming should start
type BucketsStreamCursorValueInput struct {
	CacheControl         *string    `json:"cacheControl,omitempty"`
	CreatedAt            *time.Time `json:"createdAt,omitempty"`
	DownloadExpiration   *int64     `json:"downloadExpiration,omitempty"`
	ID                   *string    `json:"id,omitempty"`
	MaxUploadFileSize    *int64     `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize    *int64     `json:"minUploadFileSize,omitempty"`
	PresignedUrlsEnabled *bool      `json:"presignedUrlsEnabled,omitempty"`
	UpdatedAt            *time.Time `json:"updatedAt,omitempty"`
}

// aggregate sum on columns
type BucketsSumFields struct {
	DownloadExpiration *int64 `json:"downloadExpiration,omitempty"`
	MaxUploadFileSize  *int64 `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *int64 `json:"minUploadFileSize,omitempty"`
}

type BucketsUpdates struct {
	// increments the numeric columns with given value of the filtered values
	Inc *BucketsIncInput `json:"_inc,omitempty"`
	// sets the columns of the filtered rows to the given values
	Set *BucketsSetInput `json:"_set,omitempty"`
	// filter the rows which have to be updated
	Where *BucketsBoolExp `json:"where"`
}

// aggregate var_pop on columns
type BucketsVarPopFields struct {
	DownloadExpiration *float64 `json:"downloadExpiration,omitempty"`
	MaxUploadFileSize  *float64 `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *float64 `json:"minUploadFileSize,omitempty"`
}

// aggregate var_samp on columns
type BucketsVarSampFields struct {
	DownloadExpiration *float64 `json:"downloadExpiration,omitempty"`
	MaxUploadFileSize  *float64 `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *float64 `json:"minUploadFileSize,omitempty"`
}

// aggregate variance on columns
type BucketsVarianceFields struct {
	DownloadExpiration *float64 `json:"downloadExpiration,omitempty"`
	MaxUploadFileSize  *float64 `json:"maxUploadFileSize,omitempty"`
	MinUploadFileSize  *float64 `json:"minUploadFileSize,omitempty"`
}

// columns and relationships of "storage.files"
type Files struct {
	// An object relationship
	Bucket           *Buckets       `json:"bucket"`
	BucketID         string         `json:"bucketId"`
	CreatedAt        time.Time      `json:"createdAt"`
	Etag             *string        `json:"etag,omitempty"`
	ID               string         `json:"id"`
	IsUploaded       *bool          `json:"isUploaded,omitempty"`
	Metadata         map[string]any `json:"metadata,omitempty"`
	MimeType         *string        `json:"mimeType,omitempty"`
	Name             *string        `json:"name,omitempty"`
	Size             *int64         `json:"size,omitempty"`
	UpdatedAt        time.Time      `json:"updatedAt"`
	UploadedByUserID *string        `json:"uploadedByUserId,omitempty"`
}

// aggregated selection of "storage.files"
type FilesAggregate struct {
	Aggregate *FilesAggregateFields `json:"aggregate,omitempty"`
	Nodes     []*Files              `json:"nodes"`
}

type FilesAggregateBoolExp struct {
	BoolAnd *FilesAggregateBoolExpBoolAnd `json:"bool_and,omitempty"`
	BoolOr  *FilesAggregateBoolExpBoolOr  `json:"bool_or,omitempty"`
	Count   *FilesAggregateBoolExpCount   `json:"count,omitempty"`
}

type FilesAggregateBoolExpBoolAnd struct {
	Arguments FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns `json:"arguments"`
	Distinct  *bool                                                         `json:"distinct,omitempty"`
	Filter    *FilesBoolExp                                                 `json:"filter,omitempty"`
	Predicate *BooleanComparisonExp                                         `json:"predicate"`
}

type FilesAggregateBoolExpBoolOr struct {
	Arguments FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns `json:"arguments"`
	Distinct  *bool                                                        `json:"distinct,omitempty"`
	Filter    *FilesBoolExp                                                `json:"filter,omitempty"`
	Predicate *BooleanComparisonExp                                        `json:"predicate"`
}

type FilesAggregateBoolExpCount struct {
	Arguments []FilesSelectColumn `json:"arguments,omitempty"`
	Distinct  *bool               `json:"distinct,omitempty"`
	Filter    *FilesBoolExp       `json:"filter,omitempty"`
	Predicate *IntComparisonExp   `json:"predicate"`
}

// aggregate fields of "storage.files"
type FilesAggregateFields struct {
	Avg        *FilesAvgFields        `json:"avg,omitempty"`
	Count      int64                  `json:"count"`
	Max        *FilesMaxFields        `json:"max,omitempty"`
	Min        *FilesMinFields        `json:"min,omitempty"`
	Stddev     *FilesStddevFields     `json:"stddev,omitempty"`
	StddevPop  *FilesStddevPopFields  `json:"stddev_pop,omitempty"`
	StddevSamp *FilesStddevSampFields `json:"stddev_samp,omitempty"`
	Sum        *FilesSumFields        `json:"sum,omitempty"`
	VarPop     *FilesVarPopFields     `json:"var_pop,omitempty"`
	VarSamp    *FilesVarSampFields    `json:"var_samp,omitempty"`
	Variance   *FilesVarianceFields   `json:"variance,omitempty"`
}

// order by aggregate values of table "storage.files"
type FilesAggregateOrderBy struct {
	Avg        *FilesAvgOrderBy        `json:"avg,omitempty"`
	Count      *OrderBy                `json:"count,omitempty"`
	Max        *FilesMaxOrderBy        `json:"max,omitempty"`
	Min        *FilesMinOrderBy        `json:"min,omitempty"`
	Stddev     *FilesStddevOrderBy     `json:"stddev,omitempty"`
	StddevPop  *FilesStddevPopOrderBy  `json:"stddev_pop,omitempty"`
	StddevSamp *FilesStddevSampOrderBy `json:"stddev_samp,omitempty"`
	Sum        *FilesSumOrderBy        `json:"sum,omitempty"`
	VarPop     *FilesVarPopOrderBy     `json:"var_pop,omitempty"`
	VarSamp    *FilesVarSampOrderBy    `json:"var_samp,omitempty"`
	Variance   *FilesVarianceOrderBy   `json:"variance,omitempty"`
}

// append existing jsonb value of filtered columns with new jsonb value
type FilesAppendInput struct {
	Metadata map[string]any `json:"metadata,omitempty"`
}

// input type for inserting array relation for remote table "storage.files"
type FilesArrRelInsertInput struct {
	Data []*FilesInsertInput `json:"data"`
	// upsert condition
	OnConflict *FilesOnConflict `json:"on_conflict,omitempty"`
}

// aggregate avg on columns
type FilesAvgFields struct {
	Size *float64 `json:"size,omitempty"`
}

// order by avg() on columns of table "storage.files"
type FilesAvgOrderBy struct {
	Size *OrderBy `json:"size,omitempty"`
}

// Boolean expression to filter rows from the table "storage.files". All fields are combined with a logical 'AND'.
type FilesBoolExp struct {
	And              []*FilesBoolExp           `json:"_and,omitempty"`
	Not              *FilesBoolExp             `json:"_not,omitempty"`
	Or               []*FilesBoolExp           `json:"_or,omitempty"`
	Bucket           *BucketsBoolExp           `json:"bucket,omitempty"`
	BucketID         *StringComparisonExp      `json:"bucketId,omitempty"`
	CreatedAt        *TimestamptzComparisonExp `json:"createdAt,omitempty"`
	Etag             *StringComparisonExp      `json:"etag,omitempty"`
	ID               *UUIDComparisonExp        `json:"id,omitempty"`
	IsUploaded       *BooleanComparisonExp     `json:"isUploaded,omitempty"`
	Metadata         *JsonbComparisonExp       `json:"metadata,omitempty"`
	MimeType         *StringComparisonExp      `json:"mimeType,omitempty"`
	Name             *StringComparisonExp      `json:"name,omitempty"`
	Size             *IntComparisonExp         `json:"size,omitempty"`
	UpdatedAt        *TimestamptzComparisonExp `json:"updatedAt,omitempty"`
	UploadedByUserID *UUIDComparisonExp        `json:"uploadedByUserId,omitempty"`
}

// delete the field or element with specified path (for JSON arrays, negative integers count from the end)
type FilesDeleteAtPathInput struct {
	Metadata []string `json:"metadata,omitempty"`
}

// delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
type FilesDeleteElemInput struct {
	Metadata *int64 `json:"metadata,omitempty"`
}

// delete key/value pair or string element. key/value pairs are matched based on their key value
type FilesDeleteKeyInput struct {
	Metadata *string `json:"metadata,omitempty"`
}

// input type for incrementing numeric columns in table "storage.files"
type FilesIncInput struct {
	Size *int64 `json:"size,omitempty"`
}

// input type for inserting data into table "storage.files"
type FilesInsertInput struct {
	Bucket           *BucketsObjRelInsertInput `json:"bucket,omitempty"`
	BucketID         *string                   `json:"bucketId,omitempty"`
	CreatedAt        *time.Time                `json:"createdAt,omitempty"`
	Etag             *string                   `json:"etag,omitempty"`
	ID               *string                   `json:"id,omitempty"`
	IsUploaded       *bool                     `json:"isUploaded,omitempty"`
	Metadata         map[string]any            `json:"metadata,omitempty"`
	MimeType         *string                   `json:"mimeType,omitempty"`
	Name             *string                   `json:"name,omitempty"`
	Size             *int64                    `json:"size,omitempty"`
	UpdatedAt        *time.Time                `json:"updatedAt,omitempty"`
	UploadedByUserID *string                   `json:"uploadedByUserId,omitempty"`
}

// aggregate max on columns
type FilesMaxFields struct {
	BucketID         *string    `json:"bucketId,omitempty"`
	CreatedAt        *time.Time `json:"createdAt,omitempty"`
	Etag             *string    `json:"etag,omitempty"`
	ID               *string    `json:"id,omitempty"`
	MimeType         *string    `json:"mimeType,omitempty"`
	Name             *string    `json:"name,omitempty"`
	Size             *int64     `json:"size,omitempty"`
	UpdatedAt        *time.Time `json:"updatedAt,omitempty"`
	UploadedByUserID *string    `json:"uploadedByUserId,omitempty"`
}

// order by max() on columns of table "storage.files"
type FilesMaxOrderBy struct {
	BucketID         *OrderBy `json:"bucketId,omitempty"`
	CreatedAt        *OrderBy `json:"createdAt,omitempty"`
	Etag             *OrderBy `json:"etag,omitempty"`
	ID               *OrderBy `json:"id,omitempty"`
	MimeType         *OrderBy `json:"mimeType,omitempty"`
	Name             *OrderBy `json:"name,omitempty"`
	Size             *OrderBy `json:"size,omitempty"`
	UpdatedAt        *OrderBy `json:"updatedAt,omitempty"`
	UploadedByUserID *OrderBy `json:"uploadedByUserId,omitempty"`
}

// aggregate min on columns
type FilesMinFields struct {
	BucketID         *string    `json:"bucketId,omitempty"`
	CreatedAt        *time.Time `json:"createdAt,omitempty"`
	Etag             *string    `json:"etag,omitempty"`
	ID               *string    `json:"id,omitempty"`
	MimeType         *string    `json:"mimeType,omitempty"`
	Name             *string    `json:"name,omitempty"`
	Size             *int64     `json:"size,omitempty"`
	UpdatedAt        *time.Time `json:"updatedAt,omitempty"`
	UploadedByUserID *string    `json:"uploadedByUserId,omitempty"`
}

// order by min() on columns of table "storage.files"
type FilesMinOrderBy struct {
	BucketID         *OrderBy `json:"bucketId,omitempty"`
	CreatedAt        *OrderBy `json:"createdAt,omitempty"`
	Etag             *OrderBy `json:"etag,omitempty"`
	ID               *OrderBy `json:"id,omitempty"`
	MimeType         *OrderBy `json:"mimeType,omitempty"`
	Name             *OrderBy `json:"name,omitempty"`
	Size             *OrderBy `json:"size,omitempty"`
	UpdatedAt        *OrderBy `json:"updatedAt,omitempty"`
	UploadedByUserID *OrderBy `json:"uploadedByUserId,omitempty"`
}

// response of any mutation on the table "storage.files"
type FilesMutationResponse struct {
	// number of rows affected by the mutation
	AffectedRows int64 `json:"affected_rows"`
	// data from the rows affected by the mutation
	Returning []*Files `json:"returning"`
}

// input type for inserting object relation for remote table "storage.files"
type FilesObjRelInsertInput struct {
	Data *FilesInsertInput `json:"data"`
	// upsert condition
	OnConflict *FilesOnConflict `json:"on_conflict,omitempty"`
}

// on_conflict condition type for table "storage.files"
type FilesOnConflict struct {
	Constraint    FilesConstraint     `json:"constraint"`
	UpdateColumns []FilesUpdateColumn `json:"update_columns"`
	Where         *FilesBoolExp       `json:"where,omitempty"`
}

// Ordering options when selecting data from "storage.files".
type FilesOrderBy struct {
	Bucket           *BucketsOrderBy `json:"bucket,omitempty"`
	BucketID         *OrderBy        `json:"bucketId,omitempty"`
	CreatedAt        *OrderBy        `json:"createdAt,omitempty"`
	Etag             *OrderBy        `json:"etag,omitempty"`
	ID               *OrderBy        `json:"id,omitempty"`
	IsUploaded       *OrderBy        `json:"isUploaded,omitempty"`
	Metadata         *OrderBy        `json:"metadata,omitempty"`
	MimeType         *OrderBy        `json:"mimeType,omitempty"`
	Name             *OrderBy        `json:"name,omitempty"`
	Size             *OrderBy        `json:"size,omitempty"`
	UpdatedAt        *OrderBy        `json:"updatedAt,omitempty"`
	UploadedByUserID *OrderBy        `json:"uploadedByUserId,omitempty"`
}

// primary key columns input for table: storage.files
type FilesPkColumnsInput struct {
	ID string `json:"id"`
}

// prepend existing jsonb value of filtered columns with new jsonb value
type FilesPrependInput struct {
	Metadata map[string]any `json:"metadata,omitempty"`
}

// input type for updating data in table "storage.files"
type FilesSetInput struct {
	BucketID         *string        `json:"bucketId,omitempty"`
	CreatedAt        *time.Time     `json:"createdAt,omitempty"`
	Etag             *string        `json:"etag,omitempty"`
	ID               *string        `json:"id,omitempty"`
	IsUploaded       *bool          `json:"isUploaded,omitempty"`
	Metadata         map[string]any `json:"metadata,omitempty"`
	MimeType         *string        `json:"mimeType,omitempty"`
	Name             *string        `json:"name,omitempty"`
	Size             *int64         `json:"size,omitempty"`
	UpdatedAt        *time.Time     `json:"updatedAt,omitempty"`
	UploadedByUserID *string        `json:"uploadedByUserId,omitempty"`
}

// aggregate stddev on columns
type FilesStddevFields struct {
	Size *float64 `json:"size,omitempty"`
}

// order by stddev() on columns of table "storage.files"
type FilesStddevOrderBy struct {
	Size *OrderBy `json:"size,omitempty"`
}

// aggregate stddev_pop on columns
type FilesStddevPopFields struct {
	Size *float64 `json:"size,omitempty"`
}

// order by stddev_pop() on columns of table "storage.files"
type FilesStddevPopOrderBy struct {
	Size *OrderBy `json:"size,omitempty"`
}

// aggregate stddev_samp on columns
type FilesStddevSampFields struct {
	Size *float64 `json:"size,omitempty"`
}

// order by stddev_samp() on columns of table "storage.files"
type FilesStddevSampOrderBy struct {
	Size *OrderBy `json:"size,omitempty"`
}

// Streaming cursor of the table "files"
type FilesStreamCursorInput struct {
	// Stream column input with initial value
	InitialValue *FilesStreamCursorValueInput `json:"initial_value"`
	// cursor ordering
	Ordering *CursorOrdering `json:"ordering,omitempty"`
}

// Initial value of the column from where the streaming should start
type FilesStreamCursorValueInput struct {
	BucketID         *string        `json:"bucketId,omitempty"`
	CreatedAt        *time.Time     `json:"createdAt,omitempty"`
	Etag             *string        `json:"etag,omitempty"`
	ID               *string        `json:"id,omitempty"`
	IsUploaded       *bool          `json:"isUploaded,omitempty"`
	Metadata         map[string]any `json:"metadata,omitempty"`
	MimeType         *string        `json:"mimeType,omitempty"`
	Name             *string        `json:"name,omitempty"`
	Size             *int64         `json:"size,omitempty"`
	UpdatedAt        *time.Time     `json:"updatedAt,omitempty"`
	UploadedByUserID *string        `json:"uploadedByUserId,omitempty"`
}

// aggregate sum on columns
type FilesSumFields struct {
	Size *int64 `json:"size,omitempty"`
}

// order by sum() on columns of table "storage.files"
type FilesSumOrderBy struct {
	Size *OrderBy `json:"size,omitempty"`
}

type FilesUpdates struct {
	// append existing jsonb value of filtered columns with new jsonb value
	Append *FilesAppendInput `json:"_append,omitempty"`
	// delete the field or element with specified path (for JSON arrays, negative integers count from the end)
	DeleteAtPath *FilesDeleteAtPathInput `json:"_delete_at_path,omitempty"`
	// delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
	DeleteElem *FilesDeleteElemInput `json:"_delete_elem,omitempty"`
	// delete key/value pair or string element. key/value pairs are matched based on their key value
	DeleteKey *FilesDeleteKeyInput `json:"_delete_key,omitempty"`
	// increments the numeric columns with given value of the filtered values
	Inc *FilesIncInput `json:"_inc,omitempty"`
	// prepend existing jsonb value of filtered columns with new jsonb value
	Prepend *FilesPrependInput `json:"_prepend,omitempty"`
	// sets the columns of the filtered rows to the given values
	Set *FilesSetInput `json:"_set,omitempty"`
	// filter the rows which have to be updated
	Where *FilesBoolExp `json:"where"`
}

// aggregate var_pop on columns
type FilesVarPopFields struct {
	Size *float64 `json:"size,omitempty"`
}

// order by var_pop() on columns of table "storage.files"
type FilesVarPopOrderBy struct {
	Size *OrderBy `json:"size,omitempty"`
}

// aggregate var_samp on columns
type FilesVarSampFields struct {
	Size *float64 `json:"size,omitempty"`
}

// order by var_samp() on columns of table "storage.files"
type FilesVarSampOrderBy struct {
	Size *OrderBy `json:"size,omitempty"`
}

// aggregate variance on columns
type FilesVarianceFields struct {
	Size *float64 `json:"size,omitempty"`
}

// order by variance() on columns of table "storage.files"
type FilesVarianceOrderBy struct {
	Size *OrderBy `json:"size,omitempty"`
}

type JsonbCastExp struct {
	String *StringComparisonExp `json:"String,omitempty"`
}

// Boolean expression to compare columns of type "jsonb". All fields are combined with logical 'AND'.
type JsonbComparisonExp struct {
	Cast *JsonbCastExp `json:"_cast,omitempty"`
	// is the column contained in the given json value
	ContainedIn map[string]any `json:"_contained_in,omitempty"`
	// does the column contain the given json value at the top level
	Contains map[string]any `json:"_contains,omitempty"`
	Eq       map[string]any `json:"_eq,omitempty"`
	Gt       map[string]any `json:"_gt,omitempty"`
	Gte      map[string]any `json:"_gte,omitempty"`
	// does the string exist as a top-level key in the column
	HasKey *string `json:"_has_key,omitempty"`
	// do all of these strings exist as top-level keys in the column
	HasKeysAll []string `json:"_has_keys_all,omitempty"`
	// do any of these strings exist as top-level keys in the column
	HasKeysAny []string         `json:"_has_keys_any,omitempty"`
	In         []map[string]any `json:"_in,omitempty"`
	IsNull     *bool            `json:"_is_null,omitempty"`
	Lt         map[string]any   `json:"_lt,omitempty"`
	Lte        map[string]any   `json:"_lte,omitempty"`
	Neq        map[string]any   `json:"_neq,omitempty"`
	Nin        []map[string]any `json:"_nin,omitempty"`
}

// mutation root
type MutationRoot struct {
}

type QueryRoot struct {
}

type SubscriptionRoot struct {
	// fetch data from the table: "storage.buckets" using primary key columns
	Bucket *Buckets `json:"bucket,omitempty"`
	// fetch data from the table: "storage.buckets"
	Buckets []*Buckets `json:"buckets"`
	// fetch aggregated fields from the table: "storage.buckets"
	BucketsAggregate *BucketsAggregate `json:"bucketsAggregate"`
	// fetch data from the table in a streaming manner: "storage.buckets"
	BucketsStream []*Buckets `json:"buckets_stream"`
	// fetch data from the table: "storage.files" using primary key columns
	File *Files `json:"file,omitempty"`
	// An array relationship
	Files []*Files `json:"files"`
	// fetch aggregated fields from the table: "storage.files"
	FilesAggregate *FilesAggregate `json:"filesAggregate"`
	// fetch data from the table in a streaming manner: "storage.files"
	FilesStream []*Files `json:"files_stream"`
	// fetch data from the table: "storage.virus" using primary key columns
	Virus *Virus `json:"virus,omitempty"`
	// fetch data from the table in a streaming manner: "storage.virus"
	VirusStream []*Virus `json:"virus_stream"`
	// fetch data from the table: "storage.virus"
	Viruses []*Virus `json:"viruses"`
	// fetch aggregated fields from the table: "storage.virus"
	VirusesAggregate *VirusAggregate `json:"virusesAggregate"`
}

// Boolean expression to compare columns of type "timestamptz". All fields are combined with logical 'AND'.
type TimestamptzComparisonExp struct {
	Eq     *time.Time   `json:"_eq,omitempty"`
	Gt     *time.Time   `json:"_gt,omitempty"`
	Gte    *time.Time   `json:"_gte,omitempty"`
	In     []*time.Time `json:"_in,omitempty"`
	IsNull *bool        `json:"_is_null,omitempty"`
	Lt     *time.Time   `json:"_lt,omitempty"`
	Lte    *time.Time   `json:"_lte,omitempty"`
	Neq    *time.Time   `json:"_neq,omitempty"`
	Nin    []*time.Time `json:"_nin,omitempty"`
}

// Boolean expression to compare columns of type "uuid". All fields are combined with logical 'AND'.
type UUIDComparisonExp struct {
	Eq     *string  `json:"_eq,omitempty"`
	Gt     *string  `json:"_gt,omitempty"`
	Gte    *string  `json:"_gte,omitempty"`
	In     []string `json:"_in,omitempty"`
	IsNull *bool    `json:"_is_null,omitempty"`
	Lt     *string  `json:"_lt,omitempty"`
	Lte    *string  `json:"_lte,omitempty"`
	Neq    *string  `json:"_neq,omitempty"`
	Nin    []string `json:"_nin,omitempty"`
}

// columns and relationships of "storage.virus"
type Virus struct {
	CreatedAt time.Time `json:"createdAt"`
	// An object relationship
	File        *Files         `json:"file"`
	FileID      string         `json:"fileId"`
	Filename    string         `json:"filename"`
	ID          string         `json:"id"`
	UpdatedAt   time.Time      `json:"updatedAt"`
	UserSession map[string]any `json:"userSession"`
	Virus       string         `json:"virus"`
}

// aggregated selection of "storage.virus"
type VirusAggregate struct {
	Aggregate *VirusAggregateFields `json:"aggregate,omitempty"`
	Nodes     []*Virus              `json:"nodes"`
}

// aggregate fields of "storage.virus"
type VirusAggregateFields struct {
	Count int64           `json:"count"`
	Max   *VirusMaxFields `json:"max,omitempty"`
	Min   *VirusMinFields `json:"min,omitempty"`
}

// append existing jsonb value of filtered columns with new jsonb value
type VirusAppendInput struct {
	UserSession map[string]any `json:"userSession,omitempty"`
}

// Boolean expression to filter rows from the table "storage.virus". All fields are combined with a logical 'AND'.
type VirusBoolExp struct {
	And         []*VirusBoolExp           `json:"_and,omitempty"`
	Not         *VirusBoolExp             `json:"_not,omitempty"`
	Or          []*VirusBoolExp           `json:"_or,omitempty"`
	CreatedAt   *TimestamptzComparisonExp `json:"createdAt,omitempty"`
	File        *FilesBoolExp             `json:"file,omitempty"`
	FileID      *UUIDComparisonExp        `json:"fileId,omitempty"`
	Filename    *StringComparisonExp      `json:"filename,omitempty"`
	ID          *UUIDComparisonExp        `json:"id,omitempty"`
	UpdatedAt   *TimestamptzComparisonExp `json:"updatedAt,omitempty"`
	UserSession *JsonbComparisonExp       `json:"userSession,omitempty"`
	Virus       *StringComparisonExp      `json:"virus,omitempty"`
}

// delete the field or element with specified path (for JSON arrays, negative integers count from the end)
type VirusDeleteAtPathInput struct {
	UserSession []string `json:"userSession,omitempty"`
}

// delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
type VirusDeleteElemInput struct {
	UserSession *int64 `json:"userSession,omitempty"`
}

// delete key/value pair or string element. key/value pairs are matched based on their key value
type VirusDeleteKeyInput struct {
	UserSession *string `json:"userSession,omitempty"`
}

// input type for inserting data into table "storage.virus"
type VirusInsertInput struct {
	CreatedAt   *time.Time              `json:"createdAt,omitempty"`
	File        *FilesObjRelInsertInput `json:"file,omitempty"`
	FileID      *string                 `json:"fileId,omitempty"`
	Filename    *string                 `json:"filename,omitempty"`
	ID          *string                 `json:"id,omitempty"`
	UpdatedAt   *time.Time              `json:"updatedAt,omitempty"`
	UserSession map[string]any          `json:"userSession,omitempty"`
	Virus       *string                 `json:"virus,omitempty"`
}

// aggregate max on columns
type VirusMaxFields struct {
	CreatedAt *time.Time `json:"createdAt,omitempty"`
	FileID    *string    `json:"fileId,omitempty"`
	Filename  *string    `json:"filename,omitempty"`
	ID        *string    `json:"id,omitempty"`
	UpdatedAt *time.Time `json:"updatedAt,omitempty"`
	Virus     *string    `json:"virus,omitempty"`
}

// aggregate min on columns
type VirusMinFields struct {
	CreatedAt *time.Time `json:"createdAt,omitempty"`
	FileID    *string    `json:"fileId,omitempty"`
	Filename  *string    `json:"filename,omitempty"`
	ID        *string    `json:"id,omitempty"`
	UpdatedAt *time.Time `json:"updatedAt,omitempty"`
	Virus     *string    `json:"virus,omitempty"`
}

// response of any mutation on the table "storage.virus"
type VirusMutationResponse struct {
	// number of rows affected by the mutation
	AffectedRows int64 `json:"affected_rows"`
	// data from the rows affected by the mutation
	Returning []*Virus `json:"returning"`
}

// on_conflict condition type for table "storage.virus"
type VirusOnConflict struct {
	Constraint    VirusConstraint     `json:"constraint"`
	UpdateColumns []VirusUpdateColumn `json:"update_columns"`
	Where         *VirusBoolExp       `json:"where,omitempty"`
}

// Ordering options when selecting data from "storage.virus".
type VirusOrderBy struct {
	CreatedAt   *OrderBy      `json:"createdAt,omitempty"`
	File        *FilesOrderBy `json:"file,omitempty"`
	FileID      *OrderBy      `json:"fileId,omitempty"`
	Filename    *OrderBy      `json:"filename,omitempty"`
	ID          *OrderBy      `json:"id,omitempty"`
	UpdatedAt   *OrderBy      `json:"updatedAt,omitempty"`
	UserSession *OrderBy      `json:"userSession,omitempty"`
	Virus       *OrderBy      `json:"virus,omitempty"`
}

// primary key columns input for table: storage.virus
type VirusPkColumnsInput struct {
	ID string `json:"id"`
}

// prepend existing jsonb value of filtered columns with new jsonb value
type VirusPrependInput struct {
	UserSession map[string]any `json:"userSession,omitempty"`
}

// input type for updating data in table "storage.virus"
type VirusSetInput struct {
	CreatedAt   *time.Time     `json:"createdAt,omitempty"`
	FileID      *string        `json:"fileId,omitempty"`
	Filename    *string        `json:"filename,omitempty"`
	ID          *string        `json:"id,omitempty"`
	UpdatedAt   *time.Time     `json:"updatedAt,omitempty"`
	UserSession map[string]any `json:"userSession,omitempty"`
	Virus       *string        `json:"virus,omitempty"`
}

// Streaming cursor of the table "virus"
type VirusStreamCursorInput struct {
	// Stream column input with initial value
	InitialValue *VirusStreamCursorValueInput `json:"initial_value"`
	// cursor ordering
	Ordering *CursorOrdering `json:"ordering,omitempty"`
}

// Initial value of the column from where the streaming should start
type VirusStreamCursorValueInput struct {
	CreatedAt   *time.Time     `json:"createdAt,omitempty"`
	FileID      *string        `json:"fileId,omitempty"`
	Filename    *string        `json:"filename,omitempty"`
	ID          *string        `json:"id,omitempty"`
	UpdatedAt   *time.Time     `json:"updatedAt,omitempty"`
	UserSession map[string]any `json:"userSession,omitempty"`
	Virus       *string        `json:"virus,omitempty"`
}

type VirusUpdates struct {
	// append existing jsonb value of filtered columns with new jsonb value
	Append *VirusAppendInput `json:"_append,omitempty"`
	// delete the field or element with specified path (for JSON arrays, negative integers count from the end)
	DeleteAtPath *VirusDeleteAtPathInput `json:"_delete_at_path,omitempty"`
	// delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
	DeleteElem *VirusDeleteElemInput `json:"_delete_elem,omitempty"`
	// delete key/value pair or string element. key/value pairs are matched based on their key value
	DeleteKey *VirusDeleteKeyInput `json:"_delete_key,omitempty"`
	// prepend existing jsonb value of filtered columns with new jsonb value
	Prepend *VirusPrependInput `json:"_prepend,omitempty"`
	// sets the columns of the filtered rows to the given values
	Set *VirusSetInput `json:"_set,omitempty"`
	// filter the rows which have to be updated
	Where *VirusBoolExp `json:"where"`
}

// unique or primary key constraints on table "storage.buckets"
type BucketsConstraint string

const (
	// unique or primary key constraint on columns "id"
	BucketsConstraintBucketsPkey BucketsConstraint = "buckets_pkey"
)

var AllBucketsConstraint = []BucketsConstraint{
	BucketsConstraintBucketsPkey,
}

func (e BucketsConstraint) IsValid() bool {
	switch e {
	case BucketsConstraintBucketsPkey:
		return true
	}
	return false
}

func (e BucketsConstraint) String() string {
	return string(e)
}

func (e *BucketsConstraint) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = BucketsConstraint(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid buckets_constraint", str)
	}
	return nil
}

func (e BucketsConstraint) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *BucketsConstraint) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e BucketsConstraint) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// select columns of table "storage.buckets"
type BucketsSelectColumn string

const (
	// column name
	BucketsSelectColumnCacheControl BucketsSelectColumn = "cacheControl"
	// column name
	BucketsSelectColumnCreatedAt BucketsSelectColumn = "createdAt"
	// column name
	BucketsSelectColumnDownloadExpiration BucketsSelectColumn = "downloadExpiration"
	// column name
	BucketsSelectColumnID BucketsSelectColumn = "id"
	// column name
	BucketsSelectColumnMaxUploadFileSize BucketsSelectColumn = "maxUploadFileSize"
	// column name
	BucketsSelectColumnMinUploadFileSize BucketsSelectColumn = "minUploadFileSize"
	// column name
	BucketsSelectColumnPresignedUrlsEnabled BucketsSelectColumn = "presignedUrlsEnabled"
	// column name
	BucketsSelectColumnUpdatedAt BucketsSelectColumn = "updatedAt"
)

var AllBucketsSelectColumn = []BucketsSelectColumn{
	BucketsSelectColumnCacheControl,
	BucketsSelectColumnCreatedAt,
	BucketsSelectColumnDownloadExpiration,
	BucketsSelectColumnID,
	BucketsSelectColumnMaxUploadFileSize,
	BucketsSelectColumnMinUploadFileSize,
	BucketsSelectColumnPresignedUrlsEnabled,
	BucketsSelectColumnUpdatedAt,
}

func (e BucketsSelectColumn) IsValid() bool {
	switch e {
	case BucketsSelectColumnCacheControl, BucketsSelectColumnCreatedAt, BucketsSelectColumnDownloadExpiration, BucketsSelectColumnID, BucketsSelectColumnMaxUploadFileSize, BucketsSelectColumnMinUploadFileSize, BucketsSelectColumnPresignedUrlsEnabled, BucketsSelectColumnUpdatedAt:
		return true
	}
	return false
}

func (e BucketsSelectColumn) String() string {
	return string(e)
}

func (e *BucketsSelectColumn) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = BucketsSelectColumn(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid buckets_select_column", str)
	}
	return nil
}

func (e BucketsSelectColumn) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *BucketsSelectColumn) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e BucketsSelectColumn) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// update columns of table "storage.buckets"
type BucketsUpdateColumn string

const (
	// column name
	BucketsUpdateColumnCacheControl BucketsUpdateColumn = "cacheControl"
	// column name
	BucketsUpdateColumnCreatedAt BucketsUpdateColumn = "createdAt"
	// column name
	BucketsUpdateColumnDownloadExpiration BucketsUpdateColumn = "downloadExpiration"
	// column name
	BucketsUpdateColumnID BucketsUpdateColumn = "id"
	// column name
	BucketsUpdateColumnMaxUploadFileSize BucketsUpdateColumn = "maxUploadFileSize"
	// column name
	BucketsUpdateColumnMinUploadFileSize BucketsUpdateColumn = "minUploadFileSize"
	// column name
	BucketsUpdateColumnPresignedUrlsEnabled BucketsUpdateColumn = "presignedUrlsEnabled"
	// column name
	BucketsUpdateColumnUpdatedAt BucketsUpdateColumn = "updatedAt"
)

var AllBucketsUpdateColumn = []BucketsUpdateColumn{
	BucketsUpdateColumnCacheControl,
	BucketsUpdateColumnCreatedAt,
	BucketsUpdateColumnDownloadExpiration,
	BucketsUpdateColumnID,
	BucketsUpdateColumnMaxUploadFileSize,
	BucketsUpdateColumnMinUploadFileSize,
	BucketsUpdateColumnPresignedUrlsEnabled,
	BucketsUpdateColumnUpdatedAt,
}

func (e BucketsUpdateColumn) IsValid() bool {
	switch e {
	case BucketsUpdateColumnCacheControl, BucketsUpdateColumnCreatedAt, BucketsUpdateColumnDownloadExpiration, BucketsUpdateColumnID, BucketsUpdateColumnMaxUploadFileSize, BucketsUpdateColumnMinUploadFileSize, BucketsUpdateColumnPresignedUrlsEnabled, BucketsUpdateColumnUpdatedAt:
		return true
	}
	return false
}

func (e BucketsUpdateColumn) String() string {
	return string(e)
}

func (e *BucketsUpdateColumn) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = BucketsUpdateColumn(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid buckets_update_column", str)
	}
	return nil
}

func (e BucketsUpdateColumn) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *BucketsUpdateColumn) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e BucketsUpdateColumn) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// ordering argument of a cursor
type CursorOrdering string

const (
	// ascending ordering of the cursor
	CursorOrderingAsc CursorOrdering = "ASC"
	// descending ordering of the cursor
	CursorOrderingDesc CursorOrdering = "DESC"
)

var AllCursorOrdering = []CursorOrdering{
	CursorOrderingAsc,
	CursorOrderingDesc,
}

func (e CursorOrdering) IsValid() bool {
	switch e {
	case CursorOrderingAsc, CursorOrderingDesc:
		return true
	}
	return false
}

func (e CursorOrdering) String() string {
	return string(e)
}

func (e *CursorOrdering) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = CursorOrdering(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid cursor_ordering", str)
	}
	return nil
}

func (e CursorOrdering) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *CursorOrdering) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e CursorOrdering) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// unique or primary key constraints on table "storage.files"
type FilesConstraint string

const (
	// unique or primary key constraint on columns "id"
	FilesConstraintFilesPkey FilesConstraint = "files_pkey"
)

var AllFilesConstraint = []FilesConstraint{
	FilesConstraintFilesPkey,
}

func (e FilesConstraint) IsValid() bool {
	switch e {
	case FilesConstraintFilesPkey:
		return true
	}
	return false
}

func (e FilesConstraint) String() string {
	return string(e)
}

func (e *FilesConstraint) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = FilesConstraint(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid files_constraint", str)
	}
	return nil
}

func (e FilesConstraint) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *FilesConstraint) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e FilesConstraint) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// select columns of table "storage.files"
type FilesSelectColumn string

const (
	// column name
	FilesSelectColumnBucketID FilesSelectColumn = "bucketId"
	// column name
	FilesSelectColumnCreatedAt FilesSelectColumn = "createdAt"
	// column name
	FilesSelectColumnEtag FilesSelectColumn = "etag"
	// column name
	FilesSelectColumnID FilesSelectColumn = "id"
	// column name
	FilesSelectColumnIsUploaded FilesSelectColumn = "isUploaded"
	// column name
	FilesSelectColumnMetadata FilesSelectColumn = "metadata"
	// column name
	FilesSelectColumnMimeType FilesSelectColumn = "mimeType"
	// column name
	FilesSelectColumnName FilesSelectColumn = "name"
	// column name
	FilesSelectColumnSize FilesSelectColumn = "size"
	// column name
	FilesSelectColumnUpdatedAt FilesSelectColumn = "updatedAt"
	// column name
	FilesSelectColumnUploadedByUserID FilesSelectColumn = "uploadedByUserId"
)

var AllFilesSelectColumn = []FilesSelectColumn{
	FilesSelectColumnBucketID,
	FilesSelectColumnCreatedAt,
	FilesSelectColumnEtag,
	FilesSelectColumnID,
	FilesSelectColumnIsUploaded,
	FilesSelectColumnMetadata,
	FilesSelectColumnMimeType,
	FilesSelectColumnName,
	FilesSelectColumnSize,
	FilesSelectColumnUpdatedAt,
	FilesSelectColumnUploadedByUserID,
}

func (e FilesSelectColumn) IsValid() bool {
	switch e {
	case FilesSelectColumnBucketID, FilesSelectColumnCreatedAt, FilesSelectColumnEtag, FilesSelectColumnID, FilesSelectColumnIsUploaded, FilesSelectColumnMetadata, FilesSelectColumnMimeType, FilesSelectColumnName, FilesSelectColumnSize, FilesSelectColumnUpdatedAt, FilesSelectColumnUploadedByUserID:
		return true
	}
	return false
}

func (e FilesSelectColumn) String() string {
	return string(e)
}

func (e *FilesSelectColumn) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = FilesSelectColumn(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid files_select_column", str)
	}
	return nil
}

func (e FilesSelectColumn) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *FilesSelectColumn) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e FilesSelectColumn) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// select "files_aggregate_bool_exp_bool_and_arguments_columns" columns of table "storage.files"
type FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns string

const (
	// column name
	FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumnsIsUploaded FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns = "isUploaded"
)

var AllFilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns = []FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns{
	FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumnsIsUploaded,
}

func (e FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns) IsValid() bool {
	switch e {
	case FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumnsIsUploaded:
		return true
	}
	return false
}

func (e FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns) String() string {
	return string(e)
}

func (e *FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid files_select_column_files_aggregate_bool_exp_bool_and_arguments_columns", str)
	}
	return nil
}

func (e FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e FilesSelectColumnFilesAggregateBoolExpBoolAndArgumentsColumns) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// select "files_aggregate_bool_exp_bool_or_arguments_columns" columns of table "storage.files"
type FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns string

const (
	// column name
	FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumnsIsUploaded FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns = "isUploaded"
)

var AllFilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns = []FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns{
	FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumnsIsUploaded,
}

func (e FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns) IsValid() bool {
	switch e {
	case FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumnsIsUploaded:
		return true
	}
	return false
}

func (e FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns) String() string {
	return string(e)
}

func (e *FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid files_select_column_files_aggregate_bool_exp_bool_or_arguments_columns", str)
	}
	return nil
}

func (e FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e FilesSelectColumnFilesAggregateBoolExpBoolOrArgumentsColumns) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// update columns of table "storage.files"
type FilesUpdateColumn string

const (
	// column name
	FilesUpdateColumnBucketID FilesUpdateColumn = "bucketId"
	// column name
	FilesUpdateColumnCreatedAt FilesUpdateColumn = "createdAt"
	// column name
	FilesUpdateColumnEtag FilesUpdateColumn = "etag"
	// column name
	FilesUpdateColumnID FilesUpdateColumn = "id"
	// column name
	FilesUpdateColumnIsUploaded FilesUpdateColumn = "isUploaded"
	// column name
	FilesUpdateColumnMetadata FilesUpdateColumn = "metadata"
	// column name
	FilesUpdateColumnMimeType FilesUpdateColumn = "mimeType"
	// column name
	FilesUpdateColumnName FilesUpdateColumn = "name"
	// column name
	FilesUpdateColumnSize FilesUpdateColumn = "size"
	// column name
	FilesUpdateColumnUpdatedAt FilesUpdateColumn = "updatedAt"
	// column name
	FilesUpdateColumnUploadedByUserID FilesUpdateColumn = "uploadedByUserId"
)

var AllFilesUpdateColumn = []FilesUpdateColumn{
	FilesUpdateColumnBucketID,
	FilesUpdateColumnCreatedAt,
	FilesUpdateColumnEtag,
	FilesUpdateColumnID,
	FilesUpdateColumnIsUploaded,
	FilesUpdateColumnMetadata,
	FilesUpdateColumnMimeType,
	FilesUpdateColumnName,
	FilesUpdateColumnSize,
	FilesUpdateColumnUpdatedAt,
	FilesUpdateColumnUploadedByUserID,
}

func (e FilesUpdateColumn) IsValid() bool {
	switch e {
	case FilesUpdateColumnBucketID, FilesUpdateColumnCreatedAt, FilesUpdateColumnEtag, FilesUpdateColumnID, FilesUpdateColumnIsUploaded, FilesUpdateColumnMetadata, FilesUpdateColumnMimeType, FilesUpdateColumnName, FilesUpdateColumnSize, FilesUpdateColumnUpdatedAt, FilesUpdateColumnUploadedByUserID:
		return true
	}
	return false
}

func (e FilesUpdateColumn) String() string {
	return string(e)
}

func (e *FilesUpdateColumn) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = FilesUpdateColumn(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid files_update_column", str)
	}
	return nil
}

func (e FilesUpdateColumn) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *FilesUpdateColumn) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e FilesUpdateColumn) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// column ordering options
type OrderBy string

const (
	// in ascending order, nulls last
	OrderByAsc OrderBy = "asc"
	// in ascending order, nulls first
	OrderByAscNullsFirst OrderBy = "asc_nulls_first"
	// in ascending order, nulls last
	OrderByAscNullsLast OrderBy = "asc_nulls_last"
	// in descending order, nulls first
	OrderByDesc OrderBy = "desc"
	// in descending order, nulls first
	OrderByDescNullsFirst OrderBy = "desc_nulls_first"
	// in descending order, nulls last
	OrderByDescNullsLast OrderBy = "desc_nulls_last"
)

var AllOrderBy = []OrderBy{
	OrderByAsc,
	OrderByAscNullsFirst,
	OrderByAscNullsLast,
	OrderByDesc,
	OrderByDescNullsFirst,
	OrderByDescNullsLast,
}

func (e OrderBy) IsValid() bool {
	switch e {
	case OrderByAsc, OrderByAscNullsFirst, OrderByAscNullsLast, OrderByDesc, OrderByDescNullsFirst, OrderByDescNullsLast:
		return true
	}
	return false
}

func (e OrderBy) String() string {
	return string(e)
}

func (e *OrderBy) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = OrderBy(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid order_by", str)
	}
	return nil
}

func (e OrderBy) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *OrderBy) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e OrderBy) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// unique or primary key constraints on table "storage.virus"
type VirusConstraint string

const (
	// unique or primary key constraint on columns "id"
	VirusConstraintVirusPkey VirusConstraint = "virus_pkey"
)

var AllVirusConstraint = []VirusConstraint{
	VirusConstraintVirusPkey,
}

func (e VirusConstraint) IsValid() bool {
	switch e {
	case VirusConstraintVirusPkey:
		return true
	}
	return false
}

func (e VirusConstraint) String() string {
	return string(e)
}

func (e *VirusConstraint) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = VirusConstraint(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid virus_constraint", str)
	}
	return nil
}

func (e VirusConstraint) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *VirusConstraint) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e VirusConstraint) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// select columns of table "storage.virus"
type VirusSelectColumn string

const (
	// column name
	VirusSelectColumnCreatedAt VirusSelectColumn = "createdAt"
	// column name
	VirusSelectColumnFileID VirusSelectColumn = "fileId"
	// column name
	VirusSelectColumnFilename VirusSelectColumn = "filename"
	// column name
	VirusSelectColumnID VirusSelectColumn = "id"
	// column name
	VirusSelectColumnUpdatedAt VirusSelectColumn = "updatedAt"
	// column name
	VirusSelectColumnUserSession VirusSelectColumn = "userSession"
	// column name
	VirusSelectColumnVirus VirusSelectColumn = "virus"
)

var AllVirusSelectColumn = []VirusSelectColumn{
	VirusSelectColumnCreatedAt,
	VirusSelectColumnFileID,
	VirusSelectColumnFilename,
	VirusSelectColumnID,
	VirusSelectColumnUpdatedAt,
	VirusSelectColumnUserSession,
	VirusSelectColumnVirus,
}

func (e VirusSelectColumn) IsValid() bool {
	switch e {
	case VirusSelectColumnCreatedAt, VirusSelectColumnFileID, VirusSelectColumnFilename, VirusSelectColumnID, VirusSelectColumnUpdatedAt, VirusSelectColumnUserSession, VirusSelectColumnVirus:
		return true
	}
	return false
}

func (e VirusSelectColumn) String() string {
	return string(e)
}

func (e *VirusSelectColumn) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = VirusSelectColumn(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid virus_select_column", str)
	}
	return nil
}

func (e VirusSelectColumn) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *VirusSelectColumn) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e VirusSelectColumn) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}

// update columns of table "storage.virus"
type VirusUpdateColumn string

const (
	// column name
	VirusUpdateColumnCreatedAt VirusUpdateColumn = "createdAt"
	// column name
	VirusUpdateColumnFileID VirusUpdateColumn = "fileId"
	// column name
	VirusUpdateColumnFilename VirusUpdateColumn = "filename"
	// column name
	VirusUpdateColumnID VirusUpdateColumn = "id"
	// column name
	VirusUpdateColumnUpdatedAt VirusUpdateColumn = "updatedAt"
	// column name
	VirusUpdateColumnUserSession VirusUpdateColumn = "userSession"
	// column name
	VirusUpdateColumnVirus VirusUpdateColumn = "virus"
)

var AllVirusUpdateColumn = []VirusUpdateColumn{
	VirusUpdateColumnCreatedAt,
	VirusUpdateColumnFileID,
	VirusUpdateColumnFilename,
	VirusUpdateColumnID,
	VirusUpdateColumnUpdatedAt,
	VirusUpdateColumnUserSession,
	VirusUpdateColumnVirus,
}

func (e VirusUpdateColumn) IsValid() bool {
	switch e {
	case VirusUpdateColumnCreatedAt, VirusUpdateColumnFileID, VirusUpdateColumnFilename, VirusUpdateColumnID, VirusUpdateColumnUpdatedAt, VirusUpdateColumnUserSession, VirusUpdateColumnVirus:
		return true
	}
	return false
}

func (e VirusUpdateColumn) String() string {
	return string(e)
}

func (e *VirusUpdateColumn) UnmarshalGQL(v any) error {
	str, ok := v.(string)
	if !ok {
		return fmt.Errorf("enums must be strings")
	}

	*e = VirusUpdateColumn(str)
	if !e.IsValid() {
		return fmt.Errorf("%s is not a valid virus_update_column", str)
	}
	return nil
}

func (e VirusUpdateColumn) MarshalGQL(w io.Writer) {
	fmt.Fprint(w, strconv.Quote(e.String()))
}

func (e *VirusUpdateColumn) UnmarshalJSON(b []byte) error {
	s, err := strconv.Unquote(string(b))
	if err != nil {
		return err
	}
	return e.UnmarshalGQL(s)
}

func (e VirusUpdateColumn) MarshalJSON() ([]byte, error) {
	var buf bytes.Buffer
	e.MarshalGQL(&buf)
	return buf.Bytes(), nil
}
